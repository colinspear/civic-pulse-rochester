{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6257c3b",
   "metadata": {},
   "source": [
    "# notebooks/pulse\\_score.ipynb\n",
    "\n",
    "**Overview**\n",
    "This notebook ingests Buffalo raw data from Athena, spatially joins to 2020 Census tracts, computes tract-level metrics (crime, vacancy, permits, licences, 311), derives a composite score, and stores results for visualization and LLM narration.\n",
    "\n",
    "---\n",
    "## 1. Load and clean data\n",
    "### 1.1 Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c548c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import awswrangler as wr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely.geometry as geom\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb23c02",
   "metadata": {},
   "source": [
    "\n",
    "Configure AWS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d06c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_REGION'] = 'us-east-1'\n",
    "wr.config.athena_workgroup = 'primary'\n",
    "DATABASE = 'civic_pulse'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20cfe5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 1.2. Load Raw Tables from Athena\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_last_n_event_days(table: str, ts_col: str, n: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pull last `n` days of events from an Athena raw_* table.\n",
    "    * Adds a partition predicate on `year` to cut the scan.\n",
    "    * Streams results in chunksize blocks to avoid S3 timeouts.\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {DATABASE}.{table}\n",
    "        WHERE {ts_col} >= date_add('day', -{n}, current_timestamp)\n",
    "          AND year >= cast(year(date_add('day', -{n}, current_timestamp)) as varchar)\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = wr.athena.read_sql_query(\n",
    "        sql,\n",
    "        database=DATABASE,\n",
    "        chunksize=200_000,\n",
    "        # unload_approach=True,\n",
    "        ctas_approach=False\n",
    "    )\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "HIST_DAYS = 365\n",
    "INDEX_DAYS = 30\n",
    "\n",
    "cut_hist   = pd.Timestamp.utcnow() - pd.Timedelta(days=HIST_DAYS)\n",
    "cut_index  = pd.Timestamp.utcnow() - pd.Timedelta(days=INDEX_DAYS)\n",
    "\n",
    "crime_df = read_last_n_event_days(\"raw_buf_crime\",  \"incident_datetime\", HIST_DAYS)\n",
    "viol_df  = read_last_n_event_days(\"raw_buf_viol\",   \"date\",              HIST_DAYS)\n",
    "# perm_df  = read_last_n_event_days(\"raw_buf_permits\",\"issued\",            HIST_DAYS)\n",
    "biz_df   = read_last_n_event_days(\"raw_buf_biz\",    \"issdttm\",           HIST_DAYS)\n",
    "calls_df = read_last_n_event_days(\"raw_buf_311\",    \"createddate\",       HIST_DAYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp patch to geocode permits data\n",
    "# TODO: update extractors for Permits and Business Licenses to:\n",
    "#   1. Pull full address information\n",
    "#   2. Geocode records\n",
    "#   3. Push geocoded records to S3 (update Athena accordingly)\n",
    "\n",
    "perm_df = pd.read_csv('permits_raw.csv')\n",
    "perm_geo = pd.read_csv('buf_permits_geocoded_all.csv')\n",
    "perm_geo['latitude'] = perm_geo['longitude'].str.split(',').str[1]\n",
    "perm_geo['longitude'] = perm_geo['longitude'].str.split(',').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_df.columns = ['id', 'apno', 'aptype', 'issued', 'address', 'value', 'pulled_utc', 'year', 'month', 'day']\n",
    "perm_df = perm_df.join(perm_geo, on='id', how='left', lsuffix='_orig', rsuffix='_geo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac7b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{(perm_df[\"latitude\"].isna().sum() / perm_df.shape[0]):.1%} of records did not return geographical coordinates.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_df.drop(['id', 'id_orig', 'Unnamed: 0', 'id_geo', 'match_ok'], axis=1, inplace=True)\n",
    "perm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crime_df[\"incident_datetime\"] = pd.to_datetime(crime_df[\"incident_datetime\"], utc=True).dt.tz_localize(None)\n",
    "viol_df[\"date\"] = pd.to_datetime(viol_df[\"date\"], utc=True).dt.tz_localize(None)\n",
    "perm_df[\"issued\"] = pd.to_datetime(perm_df[\"issued\"], utc=True).dt.tz_localize(None)\n",
    "biz_df[\"issdttm\"] = pd.to_datetime(biz_df[\"issdttm\"], utc=True).dt.tz_localize(None)\n",
    "calls_df[\"createddate\"] = pd.to_datetime(calls_df[\"createddate\"], utc=True).dt.tz_localize(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eeeb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in geocoded permit data\n",
    "perm_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFF_BOUNDS = {\n",
    "    \"lat_min\": 42.6,   # Erie Co. southern edge ≈ 42.5 °\n",
    "    \"lat_max\": 43.1,   # Tonawanda/Grand Island ≈ 43.1 °\n",
    "    \"lon_min\": -79.1,  # Hamburg/Eden ≈ -79.1 °\n",
    "    \"lon_max\": -78.4   # Clarence/Amherst ≈ -78.4 °\n",
    "}\n",
    "\n",
    "def fix_latlon(df: pd.DataFrame,\n",
    "               lat_col: str = \"latitude\",\n",
    "               lon_col: str = \"longitude\",\n",
    "               bounds: dict = BUFF_BOUNDS,\n",
    "               label: str = \"\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    • cast → numeric               • swap lat/lon where BOTH look swapped\n",
    "    • drop rows still out-of-bounds\n",
    "    \"\"\"\n",
    "\n",
    "    # numeric dtype\n",
    "    df[lat_col] = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
    "    df[lon_col] = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
    "\n",
    "    lat_in_lat = df[lat_col].between(bounds[\"lat_min\"], bounds[\"lat_max\"])\n",
    "    lon_in_lon = df[lon_col].between(bounds[\"lon_min\"], bounds[\"lon_max\"])\n",
    "\n",
    "    # both appear swapped: lat sits in lon-range AND lon sits in lat-range\n",
    "    lat_in_lon = df[lat_col].between(bounds[\"lon_min\"], bounds[\"lon_max\"])\n",
    "    lon_in_lat = df[lon_col].between(bounds[\"lat_min\"], bounds[\"lat_max\"])\n",
    "    to_swap    = (~lat_in_lat & ~lon_in_lon) & (lat_in_lon & lon_in_lat)\n",
    "\n",
    "    # irreparable rows = still out-of-bounds after potential swap\n",
    "    df.loc[to_swap, [lat_col, lon_col]] = df.loc[to_swap, [lon_col, lat_col]].values\n",
    "    lat_good = df[lat_col].between(bounds[\"lat_min\"], bounds[\"lat_max\"])\n",
    "    lon_good = df[lon_col].between(bounds[\"lon_min\"], bounds[\"lon_max\"])\n",
    "    to_drop  = ~(lat_good & lon_good)\n",
    "\n",
    "    before = len(df)\n",
    "    df.drop(index=df.index[to_drop], inplace=True)\n",
    "\n",
    "    pre = f\"{label}: \" if label else \"\"\n",
    "    print(f\"{pre}lat/lon fixed: {to_swap.sum():>6,} ({(to_swap.sum()/before):.2%} of total)\")\n",
    "    print(f\"{pre}rows dropped:  {to_drop.sum():>6,} ({(to_swap.sum()/before):.2%} of total)\")\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "crime_df = fix_latlon(crime_df, label=\"crime_df\")\n",
    "viol_df = fix_latlon(viol_df, label=\"viol_df\")\n",
    "perm_df = fix_latlon(perm_df, label=\"perm_df\")\n",
    "biz_df   = fix_latlon(biz_df,   label=\"biz_df\")\n",
    "calls_df = fix_latlon(calls_df, label=\"calls_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d344ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_df.plot.scatter(x='latitude', y='longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b3ba9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 1.3. Load 2020 Tract Shapefile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NY State tracts (state FIPS 36) from TIGER 2024\n",
    "url = (\"https://www2.census.gov/geo/tiger/TIGER2024/TRACT/\"\n",
    "       \"tl_2024_36_tract.zip\")\n",
    "\n",
    "tracts = gpd.read_file(url)[[\"GEOID\", \"geometry\"]].to_crs(epsg=4326)\n",
    "tracts = tracts[tracts[\"GEOID\"].str.startswith(\"36029\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/acs2023_erie_tract_pop.json\"   # adjust as needed\n",
    "\n",
    "with open(path) as f:\n",
    "    rows = json.load(f)        # rows[0] = header list, rows[1:] = data\n",
    "\n",
    "header, data = rows[0], rows[1:]\n",
    "acs = pd.DataFrame(data, columns=header)\n",
    "\n",
    "# numeric casts\n",
    "acs[\"population\"]     = pd.to_numeric(acs[\"B02001_001E\"], errors=\"coerce\")\n",
    "acs[\"housing_units\"]  = pd.to_numeric(acs[\"B25001_001E\"], errors=\"coerce\")\n",
    "\n",
    "# 11-digit GEOID\n",
    "acs[\"GEOID\"] = (\n",
    "    acs[\"state\"].str.zfill(2) +\n",
    "    acs[\"county\"].str.zfill(3) +\n",
    "    acs[\"tract\"].str.zfill(6)\n",
    ")\n",
    "\n",
    "acs = acs[[\"GEOID\", \"population\", \"housing_units\"]]\n",
    "\n",
    "tracts = tracts.merge(acs, on=\"GEOID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b13e08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.4. Spatial Join Points → Tracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb31d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add geocoder to extract lat/lon from addresses in permitting data\n",
    "# import requests, pandas as pd, time, json\n",
    "\n",
    "# def geocode_census(addr_series, batch=100):\n",
    "#     out = {}\n",
    "#     for chunk in addr_series.groupby(addr_series.index // batch):\n",
    "#         payload = \"\\n\".join(chunk[1].tolist())\n",
    "#         resp = requests.post(\n",
    "#             \"https://geocoding.geo.census.gov/geocoder/geographies/addressbatch\",\n",
    "#             files={\"addressFile\": (\"addrs.txt\", payload),\n",
    "#                    \"benchmark\": (None, \"Public_AR_Current\"),\n",
    "#                    \"vintage\": (None, \"Current_Current\")}\n",
    "#         )\n",
    "#         for line in resp.text.strip().split(\"\\n\"):\n",
    "#             parts = line.split(\",\")\n",
    "#             if len(parts) >= 9 and parts[8]:\n",
    "#                 out[parts[0]] = parts[8].zfill(11)   # tract GEOID\n",
    "#         time.sleep(0.2)   # stay polite\n",
    "#     return pd.Series(out, name=\"tract\")\n",
    "\n",
    "# perm_df[\"uid\"] = perm_df.index.astype(str)  # unique row key for batch file\n",
    "# perm_df[\"tract\"] = geocode_census(perm_df[\"address\"])\n",
    "\n",
    "def join_points(df, lon='longitude', lat='latitude'):\n",
    "    \"\"\"\n",
    "    If lon/lat columns are present, spatially join to tracts.\n",
    "    Otherwise just add a null 'tract' column and return the original df.\n",
    "    \"\"\"\n",
    "    if lon in df.columns and lat in df.columns:\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            df,\n",
    "            geometry=gpd.points_from_xy(df[lon], df[lat]),\n",
    "            crs='EPSG:4326'\n",
    "        )\n",
    "        out = gpd.sjoin(gdf, tracts[['GEOID', 'geometry']],\n",
    "                        how='left', predicate='within')\n",
    "        return out.drop(columns='geometry').rename(columns={'GEOID': 'tract'})\n",
    "    else:\n",
    "        df = df.copy()\n",
    "        df['tract'] = pd.NA\n",
    "        return df\n",
    "\n",
    "crime_gdf = join_points(crime_df)\n",
    "viol_gdf  = join_points(viol_df)\n",
    "perm_gdf  = join_points(perm_df)  # if no coords\n",
    "biz_gdf   = join_points(biz_df)\n",
    "calls_gdf = join_points(calls_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19191ae",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2. Pulse index\n",
    "### 2.1. Compute Tract-Level Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f09d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_recent = crime_gdf[crime_gdf[\"incident_datetime\"] >= cut_index.tz_localize(None)]\n",
    "viol_recent  = viol_gdf [viol_gdf [\"date\"]              >= cut_index.tz_localize(None)]\n",
    "perm_recent  = perm_gdf [perm_gdf [\"issued\"]            >= cut_index.tz_localize(None)]\n",
    "biz_recent   = biz_gdf  [biz_gdf  [\"issdttm\"]           >= cut_index.tz_localize(None)]\n",
    "calls_recent = calls_gdf[calls_gdf[\"createddate\"]       >= cut_index.tz_localize(None)]\n",
    "\n",
    "# initialise metrics with all tracts\n",
    "metrics = pd.DataFrame({\n",
    "    \"tract\": tracts[\"GEOID\"],\n",
    "    \"population\": tracts[\"population\"],\n",
    "    \"housing_units\": tracts[\"housing_units\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48acebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# build raw counts  (already in `metrics`)\n",
    "# --------------------------------------------------\n",
    "metrics[\"crime_cnt\"]        = crime_gdf.groupby(\"tract\").size()\n",
    "metrics[\"vacant_code_cnt\"]  = viol_gdf.groupby(\"tract\").size()\n",
    "metrics[\"permit_cnt\"]       = perm_gdf.groupby(\"tract\").size()\n",
    "metrics[\"licence_cnt\"]      = biz_gdf.groupby(\"tract\").size()\n",
    "metrics[\"calls_cnt\"]        = calls_gdf.groupby(\"tract\").size()\n",
    "metrics = metrics.fillna(0).astype({\"crime_cnt\":   \"Int64\",\n",
    "                                     \"vacant_code_cnt\": \"Int64\",\n",
    "                                     \"permit_cnt\":      \"Int64\",\n",
    "                                     \"licence_cnt\":     \"Int64\",\n",
    "                                     \"calls_cnt\":       \"Int64\"})\n",
    "\n",
    "# --------------------------------------------------\n",
    "# per-capita rates  (crime per-1 k pop, etc.)\n",
    "# --------------------------------------------------\n",
    "metrics[\"crime_rate\"]   = metrics[\"crime_cnt\"]   / (metrics[\"population\"]/1_000)\n",
    "metrics[\"vacant_rate\"]  = metrics[\"vacant_code_cnt\"] / (metrics[\"housing_units\"]/1_000)\n",
    "metrics[\"permit_rate\"]  = metrics[\"permit_cnt\"]  / (metrics[\"housing_units\"]/1_000)\n",
    "metrics[\"licence_rate\"] = metrics[\"licence_cnt\"] / (metrics[\"population\"]/1_000)\n",
    "metrics[\"calls_rate\"]   = metrics[\"calls_cnt\"]   / (metrics[\"population\"]/1_000)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# z-scores (feature list = *_rate columns)\n",
    "# --------------------------------------------------\n",
    "features = [c for c in metrics.columns if c.endswith(\"_rate\")]\n",
    "scaler   = StandardScaler()\n",
    "metrics[[f.replace(\"_rate\", \"_z\") for f in features]] = scaler.fit_transform(metrics[features])\n",
    "\n",
    "# composite pulse score\n",
    "metrics[\"score\"] = metrics[[f.replace(\"_rate\",\"_z\") for f in features]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b62b5",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Boosted tree model and SHAP scores\n",
    "### 3.1 Build weekly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# XGBoost / SHAP TRAINING PIPELINE  ── rate + count features, log/ raw target toggle\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "import numpy as np, pandas as pd, xgboost as xgb, shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 0. PARAMS\n",
    "# ------------------------------------------------------------------ #\n",
    "TARGET_MODE   = \"raw\"          # \"raw\" OR \"log\"\n",
    "TEST_WEEKS    = 4              # hold-out horizon\n",
    "CUT_NOW       = pd.Timestamp.utcnow().to_period(\"W-SUN\").start_time\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 1. Clean datetimes + remove future rows\n",
    "# ------------------------------------------------------------------ #\n",
    "for gdf, col in [\n",
    "    (crime_gdf, \"incident_datetime\"),\n",
    "    (viol_gdf,  \"date\"),\n",
    "    (perm_gdf,  \"issued\"),\n",
    "    (biz_gdf,   \"issdttm\"),\n",
    "    (calls_gdf, \"createddate\"),\n",
    "]:\n",
    "    gdf[col] = pd.to_datetime(gdf[col],  utc=True,\n",
    "                              errors=\"coerce\").dt.tz_localize(None)\n",
    "    gdf[\"week\"] = gdf[col].dt.to_period(\"W-SUN\").dt.start_time\n",
    "    gdf.drop(index=gdf.loc[gdf[\"week\"] > CUT_NOW].index, inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 2. Target table  calls_next_week  >>> weekly_y\n",
    "# ------------------------------------------------------------------ #\n",
    "weekly_y = (\n",
    "    calls_gdf.groupby([\"tract\",\"week\"]).size()\n",
    "             .rename(\"calls_next_week\").reset_index()\n",
    ")\n",
    "weekly_y[\"week\"] = weekly_y[\"week\"] - pd.Timedelta(weeks=1)   # week t predicts t+1\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 3. Predictor weekly counts        >>> weekly_pred\n",
    "# ------------------------------------------------------------------ #\n",
    "def weekly_size(df, name):\n",
    "    return df.groupby([\"tract\",\"week\"]).size().rename(name).reset_index()\n",
    "\n",
    "weekly_pred = (\n",
    "    weekly_size(crime_gdf, \"crime_count\")\n",
    "      .merge(weekly_size(viol_gdf, \"vacant_code_count\"), on=[\"tract\",\"week\"], how=\"outer\")\n",
    "      .merge(weekly_size(perm_gdf, \"permit_count\"),      on=[\"tract\",\"week\"], how=\"outer\")\n",
    "      .merge(weekly_size(biz_gdf,  \"licence_count\"),     on=[\"tract\",\"week\"], how=\"outer\")\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 4. Per-capita / per-unit rates\n",
    "# ------------------------------------------------------------------ #\n",
    "pop_hu = tracts[[\"GEOID\",\"population\",\"housing_units\"]].rename(columns={\"GEOID\":\"tract\"})\n",
    "weekly_pred = weekly_pred.merge(pop_hu, on=\"tract\", how=\"left\")\n",
    "weekly_pred[\"population\"].replace(0, np.nan, inplace=True)\n",
    "weekly_pred[\"housing_units\"].replace(0, np.nan, inplace=True)\n",
    "\n",
    "weekly_pred[\"crime_per_1k\"]   = weekly_pred[\"crime_count\"]           / (weekly_pred[\"population\"]      / 1_000)\n",
    "weekly_pred[\"vacant_per_1khu\"] = weekly_pred[\"vacant_code_count\"]     / (weekly_pred[\"housing_units\"]   / 1_000)\n",
    "weekly_pred[\"permit_per_1khu\"] = weekly_pred[\"permit_count\"]          / (weekly_pred[\"housing_units\"]   / 1_000)\n",
    "weekly_pred[\"lic_per_1khu\"]    = weekly_pred[\"licence_count\"]         / (weekly_pred[\"housing_units\"]   / 1_000)\n",
    "\n",
    "# raw + rate feature list\n",
    "feature_cols = [\n",
    "    \"crime_per_1k\", \n",
    "    \"crime_count\",\n",
    "    \"vacant_per_1khu\", \n",
    "    \"vacant_code_count\",\n",
    "    \"permit_per_1khu\", \n",
    "    \"permit_count\",\n",
    "    \"lic_per_1khu\",    \n",
    "    \"licence_count\",\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 5. Join predictors ↔ target       >>> weekly_full\n",
    "# ------------------------------------------------------------------ #\n",
    "weekly_full = weekly_pred.merge(\n",
    "    weekly_y, on=[\"tract\",\"week\"], how=\"inner\"\n",
    ").fillna(0)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 6. Train / Test split\n",
    "# ------------------------------------------------------------------ #\n",
    "cutoff = weekly_full[\"week\"].max() - pd.Timedelta(weeks=TEST_WEEKS)\n",
    "train  = weekly_full[weekly_full.week <  cutoff].set_index([\"tract\",\"week\"])\n",
    "test   = weekly_full[weekly_full.week >= cutoff].set_index([\"tract\",\"week\"])\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "X_test  = test[feature_cols]\n",
    "\n",
    "# ---------- target (raw or log) ----------\n",
    "if TARGET_MODE == \"log\":\n",
    "    y_train = np.log1p(train[\"calls_next_week\"])\n",
    "    y_test  = np.log1p(test[\"calls_next_week\"])\n",
    "else:\n",
    "    y_train = train[\"calls_next_week\"]\n",
    "    y_test  = test[\"calls_next_week\"]\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 7. Fit model\n",
    "# ------------------------------------------------------------------ #\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=300, max_depth=4, learning_rate=0.05,\n",
    "    objective=\"reg:squarederror\", subsample=0.8, colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train.values, y_train.values)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 8. SHAP values\n",
    "# ------------------------------------------------------------------ #\n",
    "explainer   = shap.Explainer(model, X_train.values, feature_names=feature_cols)\n",
    "shap_values = explainer(X_test.values)   # passes additivity by default\n",
    "\n",
    "print(f\"✅ XGB done – feature matrix {X_train.shape},  SHAP {shap_values.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26468714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "pred = model.predict(X_test.values)\n",
    "if TARGET_MODE == \"log\":\n",
    "    pred = np.expm1(pred)       # back-transform\n",
    "    y_true = np.expm1(y_test)\n",
    "else:\n",
    "    y_true = y_test\n",
    "print(\"R² =\", r2_score(y_true, pred))\n",
    "print(\"MAE=\", mean_absolute_error(y_true, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf8de7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Sanity-check suite for Civic-Pulse GBM + SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23729e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# 2. Target variance -----------------------------------------------\n",
    "results[\"var_train\"] = y_train.var()\n",
    "results[\"var_test\"]  = y_test.var()\n",
    "print(\"Target variance – train:\", results[\"var_train\"], \"test:\", results[\"var_test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Baseline vs. model RMSE ---------------------------------------\n",
    "dummy = DummyRegressor(strategy=\"mean\").fit(X_train, y_train)\n",
    "rmse_dummy = root_mean_squared_error(y_test, dummy.predict(X_test))\n",
    "rmse_xgb   = root_mean_squared_error(y_test,  model.predict(X_test))\n",
    "results[\"rmse_baseline\"] = rmse_dummy\n",
    "results[\"rmse_xgb\"]      = rmse_xgb\n",
    "print(\"RMSE  baseline:\", rmse_dummy, \"  XGB:\", rmse_xgb,\n",
    "      f\"\\nXGB {(rmse_dummy - rmse_xgb)/rmse_dummy:.02%} lower than baseline.\"\n",
    "      \"\\n(Expect XGB at least ~10 % lower)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefa49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Feature correlation heat-map ----------------------------------\n",
    "corr = pd.DataFrame(X_train, columns=feature_cols).corr()\n",
    "plt.figure(figsize=(4,3)); plt.title(\"Feature correlation (train)\")\n",
    "plt.imshow(corr, vmin=-1, vmax=1, cmap=\"coolwarm\"); plt.colorbar()\n",
    "plt.xticks(range(len(feature_cols)),feature_cols,rotation=90); plt.yticks(range(len(feature_cols)),feature_cols)\n",
    "# Eyeball: any cell >|0.95| → drop one of the twins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca0c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Gain vs. SHAP ranking -----------------------------------------\n",
    "booster   = model.get_booster()\n",
    "gain_dict = booster.get_score(importance_type='gain')\n",
    "\n",
    "# map f0, f1, … to column names in feature_cols\n",
    "gain_vec  = np.array([gain_dict.get(f\"f{i}\", 0) for i in range(len(feature_cols))])\n",
    "shap_mean = np.abs(shap_values.values).mean(axis=0)\n",
    "\n",
    "norm = lambda x: x / x.max() if x.max() else x\n",
    "df_norm = pd.DataFrame({\"gain\": norm(gain_vec), \"shap\": norm(shap_mean)},\n",
    "                       index=feature_cols).sort_values(\"shap\", ascending=False)\n",
    "df_norm.plot.bar(figsize=(6,3))\n",
    "plt.ylabel(\"Relative importance (0-1)\")\n",
    "plt.title(\"Gain vs. SHAP (normalised)\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddfa530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Additivity already checked by SHAP call -----------------------\n",
    "print(\"SHAP additivity check passed earlier ✔\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Direction sanity (scatter) ------------------------------------\n",
    "for feat in feature_cols:\n",
    "    disp = shap.plots.scatter(\n",
    "        shap_values[:, feature_cols.index(feat)],\n",
    "        color=shap_values,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f\"SHAP vs {feat}\")\n",
    "    plt.xlabel(feat); plt.ylabel(\"SHAP value\")\n",
    "    plt.show()\n",
    "    # Eyeball: monotone trend (upwards for risk-raising features).\n",
    "    # U-shapes or clouds → clipping / transformation needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f898ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Stability across cut-offs ------------------------------------\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# baseline ranking: mean(|SHAP|) on the main test set\n",
    "rank0 = pd.Series(np.abs(shap_values.values).mean(0),\n",
    "                  index=feature_cols).rank()\n",
    "\n",
    "cutoff_max = weekly_full[\"week\"].max()\n",
    "\n",
    "for w in range(2, 6):      # 2- to 5-week hold-out slices\n",
    "    cut = cutoff_max - pd.Timedelta(weeks=w)\n",
    "    subset = weekly_full[weekly_full.week >= cut][feature_cols]\n",
    "\n",
    "    if subset.empty:\n",
    "        continue           # nothing to compare\n",
    "\n",
    "    shap_subset = explainer(subset.values, check_additivity=False)\n",
    "    rank_w = pd.Series(np.abs(shap_subset.values).mean(0),\n",
    "                       index=feature_cols).rank()\n",
    "\n",
    "    rho = spearmanr(rank0, rank_w).correlation\n",
    "    print(f\"Rank corr vs. {w:>2}-week window: {rho:.2f}  (expect ≥ 0.70)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9. Permutation importance ----------------------------------------\n",
    "perm = permutation_importance(model, X_test, y_test, n_repeats=30,\n",
    "                              random_state=0)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=feature_cols)\n",
    "\n",
    "norm = lambda x: x / x.max() if x.max() else x\n",
    "\n",
    "df_norm = pd.DataFrame({\"perm\": norm(perm_imp), \"shap\": norm(shap_mean)},\n",
    "                       index=feature_cols).sort_values(\"shap\", ascending=False)\n",
    "\n",
    "df_norm.plot.bar(figsize=(6,3))\n",
    "plt.ylabel(\"Relative importance (0-1)\")\n",
    "plt.title(\"Permutation vs. SHAP\"); plt.tight_layout()\n",
    "plt.tight_layout()\n",
    "# Eyeball: top few features align ≈; divergence → investigate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10. Partial-dependence sanity ------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "PartialDependenceDisplay.from_estimator(model, X_train, [feature_cols[0]],\n",
    "                                        grid_resolution=20, ax=ax)\n",
    "plt.title(f\"PDP – {feature_cols[0]}\"); plt.tight_layout()\n",
    "# Eyeball: curve should rise logically (e.g., more crime → more 311).\n",
    "\n",
    "print(\"\\nFinished sanity checks – see plots & prints above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47204c29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Save Results to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c43621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pulse_metrics_df  (already in RAM right after compute_score)\n",
    "metrics[\"30_day_start\"] = cut_index.strftime(\"%Y-%m-%d\")\n",
    "metrics = metrics.drop(['population', 'housing_units'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88fe777",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9875dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.to_parquet(\n",
    "    metrics,\n",
    "    path=f\"s3://civic-pulse-rochester/pulse/metrics/\",\n",
    "    dataset=True,\n",
    "    partition_cols=[\"30_day_start\"],\n",
    "    mode=\"overwrite_partitions\",\n",
    "    compression=\"zstd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pulse_shap_df  (explode SHAP → long format)\n",
    "shap_df = pd.DataFrame(\n",
    "    shap_values.values,          # rows align with X_test\n",
    "    columns = feature_cols,\n",
    "    index   = test.index         # keeps the original MultiIndex\n",
    ").reset_index()[[\"tract\", \"week\", *feature_cols]]\n",
    "\n",
    "shap_long = (\n",
    "    shap_df\n",
    "    .melt(id_vars=[\"tract\", \"week\"], var_name=\"feature\", value_name=\"shap\")\n",
    ")\n",
    "shap_long['week'] = shap_long[\"week\"].dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53635152",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6342a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wr.s3.to_parquet(\n",
    "    shap_long,\n",
    "    path=f\"s3://civic-pulse-rochester/pulse/shap/\",\n",
    "    dataset=True,\n",
    "    partition_cols=[\"week\"],\n",
    "    mode=\"overwrite_partitions\",\n",
    "    compression=\"zstd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff1721",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "*Next:* Fill in ACS population load, training data for SHAP, and refine model fitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
