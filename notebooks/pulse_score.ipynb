{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6257c3b",
   "metadata": {},
   "source": [
    "# notebooks/pulse\\_score.ipynb\n",
    "\n",
    "**Overview**\n",
    "This notebook ingests Buffalo raw data from Athena, spatially joins to 2020 Census tracts, computes tract-level metrics (crime, vacancy, permits, licences, 311), derives a composite score, and stores results for visualization and LLM narration.\n",
    "\n",
    "---\n",
    "## 1. Load and clean data\n",
    "### 1.1 Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c548c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colinspear/dev/civic-pulse-rochester/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import awswrangler as wr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely.geometry as geom\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb23c02",
   "metadata": {},
   "source": [
    "\n",
    "Configure AWS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d06c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_REGION'] = 'us-east-1'\n",
    "wr.config.athena_workgroup = 'primary'\n",
    "DATABASE = 'civic_pulse'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20cfe5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 1.2. Load Raw Tables from Athena\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e4364e",
   "metadata": {},
   "outputs": [
    {
     "ename": "QueryFailed",
     "evalue": "HIVE_BAD_DATA: Field zip's type BINARY in parquet file s3://civic-pulse-rochester/raw/buf_biz/year=2024/month=11/day=08/part-0.parquet is incompatible with type integer defined in table schema",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQueryFailed\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m viol_df  \u001b[38;5;241m=\u001b[39m read_last_n_event_days(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_buf_viol\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m,              HIST_DAYS)\n\u001b[1;32m     31\u001b[0m perm_df  \u001b[38;5;241m=\u001b[39m read_last_n_event_days(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_buf_permits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missued\u001b[39m\u001b[38;5;124m\"\u001b[39m,            HIST_DAYS)\n\u001b[0;32m---> 32\u001b[0m biz_df   \u001b[38;5;241m=\u001b[39m \u001b[43mread_last_n_event_days\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw_buf_biz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43missdttm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[43mHIST_DAYS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m calls_df \u001b[38;5;241m=\u001b[39m read_last_n_event_days(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_buf_311\u001b[39m\u001b[38;5;124m\"\u001b[39m,    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreateddate\u001b[39m\u001b[38;5;124m\"\u001b[39m,       HIST_DAYS)\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mread_last_n_event_days\u001b[0;34m(table, ts_col, n)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mPull last `n` days of events from an Athena raw_* table.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m* Adds a partition predicate on `year` to cut the scan.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m* Streams results in chunksize blocks to avoid S3 timeouts.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    SELECT *\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m    FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATABASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    WHERE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mts_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m >= date_add(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, -\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, current_timestamp)\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m      AND year >= cast(year(date_add(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, -\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, current_timestamp)) as varchar)\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mwr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mathena\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATABASE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# unload_approach=True,\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_approach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(chunks, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/dev/civic-pulse-rochester/.venv/lib/python3.9/site-packages/awswrangler/_config.py:712\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[0;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m args[name]\n\u001b[1;32m    711\u001b[0m         args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords}\n\u001b[0;32m--> 712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/civic-pulse-rochester/.venv/lib/python3.9/site-packages/awswrangler/_utils.py:179\u001b[0m, in \u001b[0;36mvalidate_kwargs.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m condition_fn() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(passed_unsupported_kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInvalidArgument(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(passed_unsupported_kwargs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/civic-pulse-rochester/.venv/lib/python3.9/site-packages/awswrangler/athena/_read.py:1083\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, database, ctas_approach, unload_approach, ctas_parameters, unload_parameters, categories, chunksize, s3_output, workgroup, encryption, kms_key, keep_files, use_threads, boto3_session, client_request_token, athena_cache_settings, data_source, athena_query_wait_polling_delay, params, paramstyle, dtype_backend, s3_additional_kwargs, pyarrow_additional_kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m ctas_bucketing_info \u001b[38;5;241m=\u001b[39m ctas_parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbucketing_info\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1081\u001b[0m ctas_write_compression \u001b[38;5;241m=\u001b[39m ctas_parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resolve_query_without_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_approach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_approach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43munload_approach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munload_approach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43munload_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munload_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencryption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_database\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_temp_table_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_temp_table_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_bucketing_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_bucketing_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_write_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_write_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpyarrow_additional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpyarrow_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_request_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_request_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/civic-pulse-rochester/.venv/lib/python3.9/site-packages/awswrangler/athena/_read.py:559\u001b[0m, in \u001b[0;36m_resolve_query_without_cache\u001b[0;34m(sql, database, data_source, ctas_approach, unload_approach, unload_parameters, categories, chunksize, s3_output, workgroup, encryption, kms_key, keep_files, ctas_database, ctas_temp_table_name, ctas_bucketing_info, ctas_write_compression, athena_query_wait_polling_delay, use_threads, s3_additional_kwargs, boto3_session, pyarrow_additional_kwargs, execution_params, dtype_backend, client_request_token)\u001b[0m\n\u001b[1;32m    535\u001b[0m         unload_parameters \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _resolve_query_without_cache_unload(\n\u001b[1;32m    537\u001b[0m         sql\u001b[38;5;241m=\u001b[39msql,\n\u001b[1;32m    538\u001b[0m         file_format\u001b[38;5;241m=\u001b[39munload_parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_format\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPARQUET\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    558\u001b[0m     )\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resolve_query_without_cache_regular\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencryption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_request_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_request_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/civic-pulse-rochester/.venv/lib/python3.9/site-packages/awswrangler/athena/_read.py:451\u001b[0m, in \u001b[0;36m_resolve_query_without_cache_regular\u001b[0;34m(sql, database, data_source, s3_output, keep_files, chunksize, categories, encryption, workgroup, kms_key, use_threads, athena_query_wait_polling_delay, s3_additional_kwargs, boto3_session, execution_params, dtype_backend, client_request_token)\u001b[0m\n\u001b[1;32m    437\u001b[0m query_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m _start_query_execution(\n\u001b[1;32m    438\u001b[0m     sql\u001b[38;5;241m=\u001b[39msql,\n\u001b[1;32m    439\u001b[0m     wg_config\u001b[38;5;241m=\u001b[39mwg_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m     boto3_session\u001b[38;5;241m=\u001b[39mboto3_session,\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    450\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, query_id)\n\u001b[0;32m--> 451\u001b[0m query_metadata: _QueryMetadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_query_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_execution_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_cache_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cache_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _fetch_csv_result(\n\u001b[1;32m    460\u001b[0m     query_metadata\u001b[38;5;241m=\u001b[39mquery_metadata,\n\u001b[1;32m    461\u001b[0m     keep_files\u001b[38;5;241m=\u001b[39mkeep_files,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    467\u001b[0m )\n",
      "File \u001b[0;32m~/dev/civic-pulse-rochester/.venv/lib/python3.9/site-packages/awswrangler/athena/_utils.py:233\u001b[0m, in \u001b[0;36m_get_query_metadata\u001b[0;34m(query_execution_id, boto3_session, categories, query_execution_payload, metadata_cache_manager, athena_query_wait_polling_delay, execution_params, dtype_backend)\u001b[0m\n\u001b[1;32m    229\u001b[0m     _query_execution_payload \u001b[38;5;241m=\u001b[39m query_execution_payload\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     _query_execution_payload \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryExecutionTypeDef\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 233\u001b[0m         \u001b[43m_executions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery_execution_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_execution_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m cols_types: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m get_query_columns_types(\n\u001b[1;32m    240\u001b[0m     query_execution_id\u001b[38;5;241m=\u001b[39mquery_execution_id, boto3_session\u001b[38;5;241m=\u001b[39mboto3_session\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    242\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCasting query column types: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cols_types)\n",
      "File \u001b[0;32m~/dev/civic-pulse-rochester/.venv/lib/python3.9/site-packages/awswrangler/_config.py:712\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[0;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m args[name]\n\u001b[1;32m    711\u001b[0m         args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords}\n\u001b[0;32m--> 712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/civic-pulse-rochester/.venv/lib/python3.9/site-packages/awswrangler/athena/_executions.py:230\u001b[0m, in \u001b[0;36mwait_query\u001b[0;34m(query_execution_id, boto3_session, athena_query_wait_polling_delay)\u001b[0m\n\u001b[1;32m    228\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery state change reason: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStateChangeReason\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFAILED\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mQueryFailed(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStateChangeReason\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANCELLED\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mQueryCancelled(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStateChangeReason\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mQueryFailed\u001b[0m: HIVE_BAD_DATA: Field zip's type BINARY in parquet file s3://civic-pulse-rochester/raw/buf_biz/year=2024/month=11/day=08/part-0.parquet is incompatible with type integer defined in table schema"
     ]
    }
   ],
   "source": [
    "def read_last_n_event_days(table: str, ts_col: str, n: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pull last `n` days of events from an Athena raw_* table.\n",
    "    * Adds a partition predicate on `year` to cut the scan.\n",
    "    * Streams results in chunksize blocks to avoid S3 timeouts.\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {DATABASE}.{table}\n",
    "        WHERE {ts_col} >= date_add('day', -{n}, current_timestamp)\n",
    "          AND year >= cast(year(date_add('day', -{n}, current_timestamp)) as varchar)\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = wr.athena.read_sql_query(\n",
    "        sql,\n",
    "        database=DATABASE,\n",
    "        chunksize=200_000,\n",
    "        # unload_approach=True,\n",
    "        ctas_approach=False\n",
    "    )\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "HIST_DAYS = 365\n",
    "INDEX_DAYS = 30\n",
    "\n",
    "cut_hist   = pd.Timestamp.utcnow() - pd.Timedelta(days=HIST_DAYS)\n",
    "cut_index  = pd.Timestamp.utcnow() - pd.Timedelta(days=INDEX_DAYS)\n",
    "\n",
    "crime_df = read_last_n_event_days(\"raw_buf_crime\",  \"incident_datetime\", HIST_DAYS)\n",
    "viol_df  = read_last_n_event_days(\"raw_buf_viol\",   \"date\",              HIST_DAYS)\n",
    "perm_df  = read_last_n_event_days(\"raw_buf_permits\",\"issued\",            HIST_DAYS)\n",
    "biz_df   = read_last_n_event_days(\"raw_buf_biz\",    \"issdttm\",           HIST_DAYS)\n",
    "calls_df = read_last_n_event_days(\"raw_buf_311\",    \"createddate\",       HIST_DAYS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d4df7",
   "metadata": {},
   "source": [
    "## Temp patch to geocode permits data\n",
    "`data_ingest/geocode.py` now provide `census_batch_geocode` which is incorporated in extract scripts. The below can be deleted once new extractors have been confirmed to work properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a699803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perm_df = pd.read_csv('permits_raw.csv')\n",
    "# perm_geo = pd.read_csv('buf_permits_geocoded_all.csv')\n",
    "# perm_geo['latitude'] = perm_geo['longitude'].str.split(',').str[1]\n",
    "# perm_geo['longitude'] = perm_geo['longitude'].str.split(',').str[0]\n",
    "# perm_df.columns = ['id', 'apno', 'aptype', 'issued', 'address', 'value', 'pulled_utc', 'year', 'month', 'day']\n",
    "# perm_df = perm_df.join(perm_geo, on='id', how='left', lsuffix='_orig', rsuffix='_geo')\n",
    "# perm_df.drop(['id', 'id_orig', 'Unnamed: 0', 'id_geo', 'match_ok'], axis=1, inplace=True)\n",
    "\n",
    "# print(f'{(perm_df[\"latitude\"].isna().sum() / perm_df.shape[0]):.1%} of records did not return geographical coordinates.')\n",
    "# perm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crime_df[\"incident_datetime\"] = pd.to_datetime(crime_df[\"incident_datetime\"], utc=True).dt.tz_localize(None)\n",
    "viol_df[\"date\"] = pd.to_datetime(viol_df[\"date\"], utc=True).dt.tz_localize(None)\n",
    "perm_df[\"issued\"] = pd.to_datetime(perm_df[\"issued\"], utc=True).dt.tz_localize(None)\n",
    "biz_df[\"issdttm\"] = pd.to_datetime(biz_df[\"issdttm\"], utc=True).dt.tz_localize(None)\n",
    "calls_df[\"createddate\"] = pd.to_datetime(calls_df[\"createddate\"], utc=True).dt.tz_localize(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eeeb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in geocoded permit data\n",
    "perm_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFF_BOUNDS = {\n",
    "    \"lat_min\": 42.5,   # Erie Co. southern edge ≈ 42.5 °\n",
    "    \"lat_max\": 43.1,   # Tonawanda/Grand Island ≈ 43.1 °\n",
    "    \"lon_min\": -79.1,  # Hamburg/Eden ≈ -79.1 °\n",
    "    \"lon_max\": -78.4   # Clarence/Amherst ≈ -78.4 °\n",
    "}\n",
    "\n",
    "# TODO: move into extraction flow\n",
    "def fix_latlon(df: pd.DataFrame,\n",
    "               lat_col: str = \"latitude\",\n",
    "               lon_col: str = \"longitude\",\n",
    "               bounds: dict = BUFF_BOUNDS,\n",
    "               label: str = \"\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    • cast → numeric               • swap lat/lon where BOTH look swapped\n",
    "    • drop rows still out-of-bounds\n",
    "    \"\"\"\n",
    "\n",
    "    # numeric dtype\n",
    "    df[lat_col] = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
    "    df[lon_col] = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
    "\n",
    "    lat_in_lat = df[lat_col].between(bounds[\"lat_min\"], bounds[\"lat_max\"])\n",
    "    lon_in_lon = df[lon_col].between(bounds[\"lon_min\"], bounds[\"lon_max\"])\n",
    "\n",
    "    # both appear swapped: lat sits in lon-range AND lon sits in lat-range\n",
    "    lat_in_lon = df[lat_col].between(bounds[\"lon_min\"], bounds[\"lon_max\"])\n",
    "    lon_in_lat = df[lon_col].between(bounds[\"lat_min\"], bounds[\"lat_max\"])\n",
    "    to_swap    = (~lat_in_lat & ~lon_in_lon) & (lat_in_lon & lon_in_lat)\n",
    "\n",
    "    # irreparable rows = still out-of-bounds after potential swap\n",
    "    df.loc[to_swap, [lat_col, lon_col]] = df.loc[to_swap, [lon_col, lat_col]].values\n",
    "    lat_good = df[lat_col].between(bounds[\"lat_min\"], bounds[\"lat_max\"])\n",
    "    lon_good = df[lon_col].between(bounds[\"lon_min\"], bounds[\"lon_max\"])\n",
    "    to_drop  = ~(lat_good & lon_good)\n",
    "\n",
    "    before = len(df)\n",
    "    df.drop(index=df.index[to_drop], inplace=True)\n",
    "\n",
    "    pre = f\"{label}: \" if label else \"\"\n",
    "    print(f\"{pre}lat/lon fixed: {to_swap.sum():>6,} ({(to_swap.sum()/before):.2%} of total)\")\n",
    "    print(f\"{pre}rows dropped:  {to_drop.sum():>6,} ({(to_swap.sum()/before):.2%} of total)\")\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "crime_df = fix_latlon(crime_df, label=\"crime_df\")\n",
    "viol_df = fix_latlon(viol_df, label=\"viol_df\")\n",
    "perm_df = fix_latlon(perm_df, label=\"perm_df\")\n",
    "biz_df   = fix_latlon(biz_df,   label=\"biz_df\")\n",
    "calls_df = fix_latlon(calls_df, label=\"calls_df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b3ba9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 1.3. Load 2020 Tract Shapefile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NY State tracts (state FIPS 36) from TIGER 2024\n",
    "url = (\"https://www2.census.gov/geo/tiger/TIGER2024/TRACT/\"\n",
    "       \"tl_2024_36_tract.zip\")\n",
    "\n",
    "tracts = gpd.read_file(url)[[\"GEOID\", \"geometry\"]].to_crs(epsg=4326)\n",
    "tracts = tracts[tracts[\"GEOID\"].str.startswith(\"36029\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/acs2023_erie_tract_pop.json\"   # adjust as needed\n",
    "\n",
    "with open(path) as f:\n",
    "    rows = json.load(f)        # rows[0] = header list, rows[1:] = data\n",
    "\n",
    "header, data = rows[0], rows[1:]\n",
    "acs = pd.DataFrame(data, columns=header)\n",
    "\n",
    "# numeric casts\n",
    "acs[\"population\"]     = pd.to_numeric(acs[\"B02001_001E\"], errors=\"coerce\")\n",
    "acs[\"housing_units\"]  = pd.to_numeric(acs[\"B25001_001E\"], errors=\"coerce\")\n",
    "\n",
    "# 11-digit GEOID\n",
    "acs[\"GEOID\"] = (\n",
    "    acs[\"state\"].str.zfill(2) +\n",
    "    acs[\"county\"].str.zfill(3) +\n",
    "    acs[\"tract\"].str.zfill(6)\n",
    ")\n",
    "\n",
    "acs = acs[[\"GEOID\", \"population\", \"housing_units\"]]\n",
    "\n",
    "tracts = tracts.merge(acs, on=\"GEOID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b13e08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.4. Spatial Join Points → Tracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb31d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_points(df, lon='longitude', lat='latitude'):\n",
    "    \"\"\"\n",
    "    If lon/lat columns are present, spatially join to tracts.\n",
    "    Otherwise just add a null 'tract' column and return the original df.\n",
    "    \"\"\"\n",
    "    if lon in df.columns and lat in df.columns:\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            df,\n",
    "            geometry=gpd.points_from_xy(df[lon], df[lat]),\n",
    "            crs='EPSG:4326'\n",
    "        )\n",
    "        out = gpd.sjoin(gdf, tracts[['GEOID', 'geometry']],\n",
    "                        how='left', predicate='within')\n",
    "        return out.drop(columns='geometry').rename(columns={'GEOID': 'tract'})\n",
    "    else:\n",
    "        df = df.copy()\n",
    "        df['tract'] = pd.NA\n",
    "        return df\n",
    "\n",
    "crime_gdf = join_points(crime_df)\n",
    "viol_gdf  = join_points(viol_df)\n",
    "perm_gdf  = join_points(perm_df)\n",
    "biz_gdf   = join_points(biz_df)\n",
    "calls_gdf = join_points(calls_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19191ae",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2. Pulse index\n",
    "### 2.1. Compute Tract-Level Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f09d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_recent = crime_gdf[crime_gdf[\"incident_datetime\"] >= cut_index.tz_localize(None)]\n",
    "viol_recent  = viol_gdf [viol_gdf [\"date\"]              >= cut_index.tz_localize(None)]\n",
    "perm_recent  = perm_gdf [perm_gdf [\"issued\"]            >= cut_index.tz_localize(None)]\n",
    "biz_recent   = biz_gdf  [biz_gdf  [\"issdttm\"]           >= cut_index.tz_localize(None)]\n",
    "calls_recent = calls_gdf[calls_gdf[\"createddate\"]       >= cut_index.tz_localize(None)]\n",
    "\n",
    "# initialise metrics with all tracts\n",
    "metrics = pd.DataFrame({\n",
    "    \"tract\": tracts[\"GEOID\"],\n",
    "    \"population\": tracts[\"population\"],\n",
    "    \"housing_units\": tracts[\"housing_units\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48acebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# build raw counts  (already in `metrics`)\n",
    "# --------------------------------------------------\n",
    "metrics[\"crime_cnt\"]        = crime_gdf.groupby(\"tract\").size()\n",
    "metrics[\"vacant_code_cnt\"]  = viol_gdf.groupby(\"tract\").size()\n",
    "metrics[\"permit_cnt\"]       = perm_gdf.groupby(\"tract\").size()\n",
    "metrics[\"licence_cnt\"]      = biz_gdf.groupby(\"tract\").size()\n",
    "metrics[\"calls_cnt\"]        = calls_gdf.groupby(\"tract\").size()\n",
    "metrics = metrics.fillna(0).astype({\"crime_cnt\":   \"Int64\",\n",
    "                                     \"vacant_code_cnt\": \"Int64\",\n",
    "                                     \"permit_cnt\":      \"Int64\",\n",
    "                                     \"licence_cnt\":     \"Int64\",\n",
    "                                     \"calls_cnt\":       \"Int64\"})\n",
    "\n",
    "# --------------------------------------------------\n",
    "# per-capita rates  (crime per-1 k pop, etc.)\n",
    "# --------------------------------------------------\n",
    "metrics[\"crime_rate\"]   = metrics[\"crime_cnt\"]   / (metrics[\"population\"]/1_000)\n",
    "metrics[\"vacant_rate\"]  = metrics[\"vacant_code_cnt\"] / (metrics[\"housing_units\"]/1_000)\n",
    "metrics[\"permit_rate\"]  = metrics[\"permit_cnt\"]  / (metrics[\"housing_units\"]/1_000)\n",
    "metrics[\"licence_rate\"] = metrics[\"licence_cnt\"] / (metrics[\"population\"]/1_000)\n",
    "metrics[\"calls_rate\"]   = metrics[\"calls_cnt\"]   / (metrics[\"population\"]/1_000)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# z-scores (feature list = *_rate columns)\n",
    "# --------------------------------------------------\n",
    "features = [c for c in metrics.columns if c.endswith(\"_rate\")]\n",
    "scaler   = StandardScaler()\n",
    "metrics[[f.replace(\"_rate\", \"_z\") for f in features]] = scaler.fit_transform(metrics[features])\n",
    "\n",
    "# composite pulse score\n",
    "metrics[\"score\"] = metrics[[f.replace(\"_rate\",\"_z\") for f in features]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b62b5",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Boosted tree model and SHAP scores\n",
    "### 3.1 Build weekly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# XGBoost / SHAP TRAINING PIPELINE  ── rate + count features, log/ raw target toggle\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "import numpy as np, pandas as pd, xgboost as xgb, shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 0. PARAMS\n",
    "# ------------------------------------------------------------------ #\n",
    "TARGET_MODE   = \"raw\"          # \"raw\" OR \"log\"\n",
    "TEST_WEEKS    = 4              # hold-out horizon\n",
    "CUT_NOW       = pd.Timestamp.utcnow().to_period(\"W-SUN\").start_time\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 1. Clean datetimes + remove future rows\n",
    "# ------------------------------------------------------------------ #\n",
    "for gdf, col in [\n",
    "    (crime_gdf, \"incident_datetime\"),\n",
    "    (viol_gdf,  \"date\"),\n",
    "    (perm_gdf,  \"issued\"),\n",
    "    (biz_gdf,   \"issdttm\"),\n",
    "    (calls_gdf, \"createddate\"),\n",
    "]:\n",
    "    gdf[col] = pd.to_datetime(gdf[col],  utc=True,\n",
    "                              errors=\"coerce\").dt.tz_localize(None)\n",
    "    gdf[\"week\"] = gdf[col].dt.to_period(\"W-SUN\").dt.start_time\n",
    "    gdf.drop(index=gdf.loc[gdf[\"week\"] > CUT_NOW].index, inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 2. Target table  calls_next_week  >>> weekly_y\n",
    "# ------------------------------------------------------------------ #\n",
    "weekly_y = (\n",
    "    calls_gdf.groupby([\"tract\",\"week\"]).size()\n",
    "             .rename(\"calls_next_week\").reset_index()\n",
    ")\n",
    "weekly_y[\"week\"] = weekly_y[\"week\"] - pd.Timedelta(weeks=1)   # week t predicts t+1\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 3. Predictor weekly counts        >>> weekly_pred\n",
    "# ------------------------------------------------------------------ #\n",
    "def weekly_size(df, name):\n",
    "    return df.groupby([\"tract\",\"week\"]).size().rename(name).reset_index()\n",
    "\n",
    "weekly_pred = (\n",
    "    weekly_size(crime_gdf, \"crime_count\")\n",
    "      .merge(weekly_size(viol_gdf, \"vacant_code_count\"), on=[\"tract\",\"week\"], how=\"outer\")\n",
    "      .merge(weekly_size(perm_gdf, \"permit_count\"),      on=[\"tract\",\"week\"], how=\"outer\")\n",
    "      .merge(weekly_size(biz_gdf,  \"licence_count\"),     on=[\"tract\",\"week\"], how=\"outer\")\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 4. Per-capita / per-unit rates\n",
    "# ------------------------------------------------------------------ #\n",
    "pop_hu = tracts[[\"GEOID\",\"population\",\"housing_units\"]].rename(columns={\"GEOID\":\"tract\"})\n",
    "weekly_pred = weekly_pred.merge(pop_hu, on=\"tract\", how=\"left\")\n",
    "weekly_pred[\"population\"].replace(0, np.nan, inplace=True)\n",
    "weekly_pred[\"housing_units\"].replace(0, np.nan, inplace=True)\n",
    "\n",
    "weekly_pred[\"crime_per_1k\"]   = weekly_pred[\"crime_count\"]           / (weekly_pred[\"population\"]      / 1_000)\n",
    "weekly_pred[\"vacant_per_1khu\"] = weekly_pred[\"vacant_code_count\"]     / (weekly_pred[\"housing_units\"]   / 1_000)\n",
    "weekly_pred[\"permit_per_1khu\"] = weekly_pred[\"permit_count\"]          / (weekly_pred[\"housing_units\"]   / 1_000)\n",
    "weekly_pred[\"lic_per_1khu\"]    = weekly_pred[\"licence_count\"]         / (weekly_pred[\"housing_units\"]   / 1_000)\n",
    "\n",
    "# raw + rate feature list\n",
    "feature_cols = [\n",
    "    \"crime_per_1k\", \n",
    "    \"crime_count\",\n",
    "    \"vacant_per_1khu\", \n",
    "    \"vacant_code_count\",\n",
    "    \"permit_per_1khu\", \n",
    "    \"permit_count\",\n",
    "    \"lic_per_1khu\",    \n",
    "    \"licence_count\",\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 5. Join predictors ↔ target       >>> weekly_full\n",
    "# ------------------------------------------------------------------ #\n",
    "weekly_full = weekly_pred.merge(\n",
    "    weekly_y, on=[\"tract\",\"week\"], how=\"inner\"\n",
    ").fillna(0)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 6. Train / Test split\n",
    "# ------------------------------------------------------------------ #\n",
    "cutoff = weekly_full[\"week\"].max() - pd.Timedelta(weeks=TEST_WEEKS)\n",
    "train  = weekly_full[weekly_full.week <  cutoff].set_index([\"tract\",\"week\"])\n",
    "test   = weekly_full[weekly_full.week >= cutoff].set_index([\"tract\",\"week\"])\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "X_test  = test[feature_cols]\n",
    "\n",
    "# ---------- target (raw or log) ----------\n",
    "if TARGET_MODE == \"log\":\n",
    "    y_train = np.log1p(train[\"calls_next_week\"])\n",
    "    y_test  = np.log1p(test[\"calls_next_week\"])\n",
    "else:\n",
    "    y_train = train[\"calls_next_week\"]\n",
    "    y_test  = test[\"calls_next_week\"]\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 7. Fit model\n",
    "# ------------------------------------------------------------------ #\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=300, max_depth=4, learning_rate=0.05,\n",
    "    objective=\"reg:squarederror\", subsample=0.8, colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train.values, y_train.values)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 8. SHAP values\n",
    "# ------------------------------------------------------------------ #\n",
    "explainer   = shap.Explainer(model, X_train.values, feature_names=feature_cols)\n",
    "shap_values = explainer(X_test.values)   # passes additivity by default\n",
    "\n",
    "print(f\"✅ XGB done – feature matrix {X_train.shape},  SHAP {shap_values.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26468714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "pred = model.predict(X_test.values)\n",
    "if TARGET_MODE == \"log\":\n",
    "    pred = np.expm1(pred)       # back-transform\n",
    "    y_true = np.expm1(y_test)\n",
    "else:\n",
    "    y_true = y_test\n",
    "print(\"R² =\", r2_score(y_true, pred))\n",
    "print(\"MAE=\", mean_absolute_error(y_true, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf8de7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Sanity-check suite for Civic-Pulse GBM + SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23729e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# 2. Target variance -----------------------------------------------\n",
    "results[\"var_train\"] = y_train.var()\n",
    "results[\"var_test\"]  = y_test.var()\n",
    "print(\"Target variance – train:\", results[\"var_train\"], \"test:\", results[\"var_test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Baseline vs. model RMSE ---------------------------------------\n",
    "dummy = DummyRegressor(strategy=\"mean\").fit(X_train, y_train)\n",
    "rmse_dummy = root_mean_squared_error(y_test, dummy.predict(X_test))\n",
    "rmse_xgb   = root_mean_squared_error(y_test,  model.predict(X_test))\n",
    "results[\"rmse_baseline\"] = rmse_dummy\n",
    "results[\"rmse_xgb\"]      = rmse_xgb\n",
    "print(\"RMSE  baseline:\", rmse_dummy, \"  XGB:\", rmse_xgb,\n",
    "      f\"\\nXGB {(rmse_dummy - rmse_xgb)/rmse_dummy:.02%} lower than baseline.\"\n",
    "      \"\\n(Expect XGB at least ~10 % lower)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefa49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Feature correlation heat-map ----------------------------------\n",
    "corr = pd.DataFrame(X_train, columns=feature_cols).corr()\n",
    "plt.figure(figsize=(4,3)); plt.title(\"Feature correlation (train)\")\n",
    "plt.imshow(corr, vmin=-1, vmax=1, cmap=\"coolwarm\"); plt.colorbar()\n",
    "plt.xticks(range(len(feature_cols)),feature_cols,rotation=90); plt.yticks(range(len(feature_cols)),feature_cols)\n",
    "# Eyeball: any cell >|0.95| → drop one of the twins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca0c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Gain vs. SHAP ranking -----------------------------------------\n",
    "booster   = model.get_booster()\n",
    "gain_dict = booster.get_score(importance_type='gain')\n",
    "\n",
    "# map f0, f1, … to column names in feature_cols\n",
    "gain_vec  = np.array([gain_dict.get(f\"f{i}\", 0) for i in range(len(feature_cols))])\n",
    "shap_mean = np.abs(shap_values.values).mean(axis=0)\n",
    "\n",
    "norm = lambda x: x / x.max() if x.max() else x\n",
    "df_norm = pd.DataFrame({\"gain\": norm(gain_vec), \"shap\": norm(shap_mean)},\n",
    "                       index=feature_cols).sort_values(\"shap\", ascending=False)\n",
    "df_norm.plot.bar(figsize=(6,3))\n",
    "plt.ylabel(\"Relative importance (0-1)\")\n",
    "plt.title(\"Gain vs. SHAP (normalised)\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddfa530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Additivity already checked by SHAP call -----------------------\n",
    "print(\"SHAP additivity check passed earlier ✔\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Direction sanity (scatter) ------------------------------------\n",
    "for feat in feature_cols:\n",
    "    disp = shap.plots.scatter(\n",
    "        shap_values[:, feature_cols.index(feat)],\n",
    "        color=shap_values,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f\"SHAP vs {feat}\")\n",
    "    plt.xlabel(feat); plt.ylabel(\"SHAP value\")\n",
    "    plt.show()\n",
    "    # Eyeball: monotone trend (upwards for risk-raising features).\n",
    "    # U-shapes or clouds → clipping / transformation needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f898ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Stability across cut-offs ------------------------------------\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# baseline ranking: mean(|SHAP|) on the main test set\n",
    "rank0 = pd.Series(np.abs(shap_values.values).mean(0),\n",
    "                  index=feature_cols).rank()\n",
    "\n",
    "cutoff_max = weekly_full[\"week\"].max()\n",
    "\n",
    "for w in range(2, 6):      # 2- to 5-week hold-out slices\n",
    "    cut = cutoff_max - pd.Timedelta(weeks=w)\n",
    "    subset = weekly_full[weekly_full.week >= cut][feature_cols]\n",
    "\n",
    "    if subset.empty:\n",
    "        continue           # nothing to compare\n",
    "\n",
    "    shap_subset = explainer(subset.values, check_additivity=False)\n",
    "    rank_w = pd.Series(np.abs(shap_subset.values).mean(0),\n",
    "                       index=feature_cols).rank()\n",
    "\n",
    "    rho = spearmanr(rank0, rank_w).correlation\n",
    "    print(f\"Rank corr vs. {w:>2}-week window: {rho:.2f}  (expect ≥ 0.70)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9. Permutation importance ----------------------------------------\n",
    "perm = permutation_importance(model, X_test, y_test, n_repeats=30,\n",
    "                              random_state=0)\n",
    "perm_imp = pd.Series(perm.importances_mean, index=feature_cols)\n",
    "\n",
    "norm = lambda x: x / x.max() if x.max() else x\n",
    "\n",
    "df_norm = pd.DataFrame({\"perm\": norm(perm_imp), \"shap\": norm(shap_mean)},\n",
    "                       index=feature_cols).sort_values(\"shap\", ascending=False)\n",
    "\n",
    "df_norm.plot.bar(figsize=(6,3))\n",
    "plt.ylabel(\"Relative importance (0-1)\")\n",
    "plt.title(\"Permutation vs. SHAP\"); plt.tight_layout()\n",
    "plt.tight_layout()\n",
    "# Eyeball: top few features align ≈; divergence → investigate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10. Partial-dependence sanity ------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "PartialDependenceDisplay.from_estimator(model, X_train, [feature_cols[0]],\n",
    "                                        grid_resolution=20, ax=ax)\n",
    "plt.title(f\"PDP – {feature_cols[0]}\"); plt.tight_layout()\n",
    "# Eyeball: curve should rise logically (e.g., more crime → more 311).\n",
    "\n",
    "print(\"\\nFinished sanity checks – see plots & prints above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47204c29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Save Results to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c43621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pulse_metrics_df  (already in RAM right after compute_score)\n",
    "metrics[\"30_day_start\"] = cut_index.strftime(\"%Y-%m-%d\")\n",
    "metrics = metrics.drop(['population', 'housing_units'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88fe777",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9875dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.to_parquet(\n",
    "    metrics,\n",
    "    path=f\"s3://civic-pulse-rochester/pulse/metrics/\",\n",
    "    dataset=True,\n",
    "    partition_cols=[\"30_day_start\"],\n",
    "    mode=\"overwrite_partitions\",\n",
    "    compression=\"zstd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pulse_shap_df  (explode SHAP → long format)\n",
    "shap_df = pd.DataFrame(\n",
    "    shap_values.values,          # rows align with X_test\n",
    "    columns = feature_cols,\n",
    "    index   = test.index         # keeps the original MultiIndex\n",
    ").reset_index()[[\"tract\", \"week\", *feature_cols]]\n",
    "\n",
    "shap_long = (\n",
    "    shap_df\n",
    "    .melt(id_vars=[\"tract\", \"week\"], var_name=\"feature\", value_name=\"shap\")\n",
    ")\n",
    "shap_long['week'] = shap_long[\"week\"].dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53635152",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6342a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wr.s3.to_parquet(\n",
    "    shap_long,\n",
    "    path=f\"s3://civic-pulse-rochester/pulse/shap/\",\n",
    "    dataset=True,\n",
    "    partition_cols=[\"week\"],\n",
    "    mode=\"overwrite_partitions\",\n",
    "    compression=\"zstd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff1721",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "*Next:* Fill in ACS population load, training data for SHAP, and refine model fitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
